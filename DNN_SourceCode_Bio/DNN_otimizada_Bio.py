import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import keras_tuner as kt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt


# Carregar o arquivo CSV
dataset = pd.read_csv('binaryfingerprints_BIO_FEW.csv', encoding='latin-1', sep=',')



# Remover outliers da vari치vel alvo 'logP'
#Q1 = dataset['logP'].quantile(0.25)
#Q3 = dataset['logP'].quantile(0.75)
#IQR = Q3 - Q1
#lower_bound = Q1 - 1.5 * IQR
#upper_bound = Q3 + 1.5 * IQR
#dataset = dataset[(dataset['logP'] >= lower_bound) & (dataset['logP'] <= upper_bound)]

# Sele칞칚o das features (todas exceto a 칰ltima coluna)
features = dataset.columns[:-1]

# Defini칞칚o das vari치veis X (features) e y (vari치vel alvo)
X = dataset[features]
y = dataset['Bioavailability']

# Normaliza칞칚o das features usando RobustScaler
#scaler = RobustScaler()
#X_scaled = scaler.fit_transform(X)

# Divis칚o dos dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 游댳 Fun칞칚o para constru칞칚o do modelo com hiperpar칙metros vari치veis
def build_model(hp):
    model = keras.Sequential()
    model.add(keras.layers.Dense(hp.Int('units_1', min_value=64, max_value=256, step=32),
                                 activation='relu',
                                 input_shape=(X_train.shape[1],)))

    # Adicionar camadas ocultas vari치veis (1 a 3 camadas)
    for i in range(hp.Int('num_layers', 1, 3)):
        model.add(keras.layers.Dense(hp.Int(f'units_{i+2}', min_value=32, max_value=128, step=32),
                                     activation='relu'))

    model.add(keras.layers.Dense(1))  # Sa칤da 칰nica para classifica칞칚o

    # Escolher taxa de aprendizado ideal
    model.compile(optimizer=keras.optimizers.Adam(
                      learning_rate=hp.Choice('learning_rate', [0.01, 0.001, 0.0001])),
                  loss='binary_crossentropy',
                  metrics=['accuracy', keras.metrics.AUC(name='auc')])
    return model

# 游댳 Criar o otimizador de hiperpar칙metros
tuner = kt.BayesianOptimization(
    build_model,
    objective='val_loss',
    max_trials=10,  # N칰mero de modelos a testar
    directory='~/helderDL/DNN/BioFew',
    project_name='ANN_hyperparam_tuning'
)

# 游댳 Procurar pelos melhores hiperpar칙metros
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=1)

# 游댳 Obter o melhor modelo encontrado
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
best_model = tuner.hypermodel.build(best_hps)

# 游댳 Treinar o modelo final com os melhores hiperpar칙metros
history = best_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1)


results = best_model.evaluate(X_test, y_test, verbose=0)
print("test loss, test acc:", results)

# Previs칚o nos dados de teste
y_pred = best_model.predict(X_test).flatten()
y_pred = np.round_(y_pred)

# Avalia칞칚o do modelo
accuracy = accuracy_score(y_test, y_pred)
#f1 = f1_score(y_test, y_pred)
#auc = roc_auc_score(y_test, y_pred)
print(confusion_matrix(y_test, y_pred))

# Exibir as m칠tricas
print(f"Accuracy: {accuracy}")
#print(f"F1-Score: {f1}")
#print(f"AUC: {auc}")

print("Relat칩rio de Classifica칞칚o:\n", classification_report(y_test, y_pred_classes))
print("Matriz de Confus칚o:\n", confusion_matrix(y_test, y_pred_classes))
